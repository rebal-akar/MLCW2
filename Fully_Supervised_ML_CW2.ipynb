{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebal-akar/MLCW2/blob/main/Fully_Supervised_ML_CW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUGjv_zRMGNe"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    import sklearn\n",
        "    import matplotlib\n",
        "except ImportError:\n",
        "    !pip install torch torchvision scikit-learn matplotlib tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fully Supverised 2\n"
      ],
      "metadata": {
        "id": "X9EVxrbjy77I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 1 â€“ Imports & Setup\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "Dc7V-4o41Qa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n"
      ],
      "metadata": {
        "id": "yuh3T7Ac1R2S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load Pretrained SimCLR Model for Selection\n",
        "python\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "ELUjb0MoEnOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "\n",
        "def load_pretrained_simclr_model(model_path, device):\n",
        "    class SimCLRModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimCLRModel, self).__init__()\n",
        "            resnet = models.resnet18(pretrained=False)\n",
        "            # Modify for CIFAR-10: use 3x3 conv and remove maxpool.\n",
        "            resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            resnet.maxpool = nn.Identity()\n",
        "            resnet.fc = nn.Identity()  # We'll only use the encoder part.\n",
        "            self.encoder = resnet\n",
        "            # Projection head is used only during pre-training.\n",
        "            self.projector = nn.Sequential(\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 128)\n",
        "            )\n",
        "\n",
        "        def forward(self, x, return_embedding=False):\n",
        "            h = self.encoder(x)\n",
        "            h = torch.flatten(h, 1)\n",
        "            if return_embedding:\n",
        "                return h  # Use these embeddings for selection.\n",
        "            return self.projector(h)\n",
        "\n",
        "    simclr_model = SimCLRModel().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    new_state_dict = OrderedDict()\n",
        "    for key, value in checkpoint.items():\n",
        "        if key.startswith(\"backbone.\"):\n",
        "            new_key = \"encoder.\" + key[len(\"backbone.\"):]\n",
        "        elif key.startswith(\"contrastive_head.\"):\n",
        "            new_key = \"projector.\" + key[len(\"contrastive_head.\"):]\n",
        "        else:\n",
        "            new_key = key\n",
        "        if \"shortcut\" in new_key:\n",
        "            new_key = new_key.replace(\"shortcut\", \"downsample\")\n",
        "        new_state_dict[new_key] = value\n",
        "    missing_keys, unexpected_keys = simclr_model.load_state_dict(new_state_dict, strict=False)\n",
        "    print(\"Missing keys:\", missing_keys)\n",
        "    print(\"Unexpected keys:\", unexpected_keys)\n",
        "    simclr_model.eval()  # Freeze the SimCLR model for selection.\n",
        "    print(\"âœ… Pretrained SimCLR model loaded successfully!\")\n",
        "    return simclr_model\n",
        "\n",
        "# Example usage:\n",
        "simclr_ckpt_path = \"/content/drive/MyDrive/Typiclust/simclr_cifar-10.pth\"\n",
        "simclr_model = load_pretrained_simclr_model(simclr_ckpt_path, device)\n"
      ],
      "metadata": {
        "id": "ts3oxmelEjaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8f633a-5957-4f91-d16a-c5848f478efa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys: []\n",
            "Unexpected keys: []\n",
            "âœ… Pretrained SimCLR model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 â€“ Data Preparation\n"
      ],
      "metadata": {
        "id": "8Ti-OwvD1WQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n"
      ],
      "metadata": {
        "id": "a07jm51U1XVD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 â€“ ResNet18 Model for CIFAR-10\n",
        "\n"
      ],
      "metadata": {
        "id": "U3Rgq_CS1UPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet18_fully_supervised(num_classes=10):\n",
        "    model = torchvision.models.resnet18(weights=None)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    model.fc = nn.Linear(512, num_classes)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "9kuA8XMv1Rbj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 5 â€“ Training, Evaluation, Embedding Extraction\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "6jEP0euM1Zyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_clusters = 500\n"
      ],
      "metadata": {
        "id": "xkO-Y_q61Y8O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, num_epochs=100):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            scheduler.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            avg_loss = running_loss / len(train_loader.dataset)\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def extract_embeddings(model, loader):\n",
        "    model.eval()\n",
        "    feat_model = copy.deepcopy(model)\n",
        "    feat_model.fc = nn.Identity()\n",
        "    feat_model = feat_model.to(device)\n",
        "\n",
        "    all_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            images = images.to(device)\n",
        "            feats = feat_model(images, return_embedding=True).cpu()\n",
        "            norms = np.linalg.norm(feats, axis=1, keepdims=True) + 1e-8\n",
        "            feats = feats / norms\n",
        "            all_embeddings.append(feats.cpu().numpy())\n",
        "    return np.concatenate(all_embeddings, axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "jXs2awbh1aYo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6 â€“ TypiClust Selection\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "-oRNGzc21bU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_typicality_nn(cluster_features, K=20):\n",
        "    \"\"\"\n",
        "    Compute typicality scores for a set of feature vectors.\n",
        "    Typicality is defined as the inverse of the average distance\n",
        "    (excluding self-distance) to K nearest neighbors.\n",
        "    \"\"\"\n",
        "    n = cluster_features.shape[0]\n",
        "    if n == 0:\n",
        "        return np.array([])\n",
        "    nbrs = NearestNeighbors(n_neighbors=min(K + 1, n), algorithm='auto', metric='euclidean').fit(cluster_features)\n",
        "    distances, _ = nbrs.kneighbors(cluster_features)\n",
        "    # Exclude the zero distance to self (first column)\n",
        "    avg_distances = np.mean(distances[:, 1:], axis=1)\n",
        "    typicality = 1.0 / (avg_distances + 1e-8)\n",
        "    return typicality\n",
        "\n",
        "def typiclust_pool_selection_batch(unlabeled_embeddings, unlabeled_ids, budget, max_clusters, labeled_embeddings, labeled_ids, min_cluster_size=5,random_state=None):\n",
        "    \"\"\"\n",
        "    Select a batch of indices from the unlabeled pool using TypiClust.\n",
        "\n",
        "    This function clusters the embeddings using KMeans or MiniBatchKMeans,\n",
        "    then for clusters that have no labeled data and are large enough,\n",
        "    selects the most \"typical\" sample based on the inverse of the average\n",
        "    distance to its K nearest neighbors.\n",
        "\n",
        "    Args:\n",
        "        unlabeled_embeddings (np.ndarray): Embeddings for unlabeled data.\n",
        "        unlabeled_ids (list): The corresponding original indices for unlabeled data.\n",
        "        budget (int): Number of samples to select.\n",
        "        max_clusters (int): Maximum number of clusters to form.\n",
        "        labeled_embeddings (np.ndarray): Embeddings for already labeled data.\n",
        "        labeled_ids (list): The indices of labeled data.\n",
        "        min_cluster_size (int): Minimum number of unlabeled samples in a cluster to be considered.\n",
        "\n",
        "    Returns:\n",
        "        queries (list): List of selected sample indices from unlabeled_ids.\n",
        "    \"\"\"\n",
        "    current_labeled_count = len(labeled_ids)\n",
        "    n_clusters = min(current_labeled_count + budget, max_clusters)\n",
        "\n",
        "    if current_labeled_count > 0:\n",
        "        X_total = np.concatenate([unlabeled_embeddings, labeled_embeddings], axis=0)\n",
        "        is_unlabeled = np.array([True] * unlabeled_embeddings.shape[0] + [False] * labeled_embeddings.shape[0])\n",
        "    else:\n",
        "        X_total = unlabeled_embeddings\n",
        "        is_unlabeled = np.array([True] * unlabeled_embeddings.shape[0])\n",
        "\n",
        "    if n_clusters > 50:\n",
        "        clusterer = MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state)\n",
        "    else:\n",
        "        clusterer = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
        "    cluster_labels = clusterer.fit_predict(X_total)\n",
        "\n",
        "    # Gather cluster info: unlabeled indices and labeled count per cluster.\n",
        "    cluster_stats = []\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        idxs = np.where(cluster_labels == cluster)[0]\n",
        "        cluster_size = len(idxs)\n",
        "        labeled_count = np.sum(~is_unlabeled[idxs])\n",
        "        unlabeled_idxs = [i for i in idxs if is_unlabeled[i]]\n",
        "        if len(unlabeled_idxs) >= min_cluster_size:\n",
        "            cluster_stats.append({\n",
        "                'cluster_id': cluster,\n",
        "                'cluster_size': cluster_size,\n",
        "                'labeled_count': labeled_count,\n",
        "                'unlabeled_idx': unlabeled_idxs\n",
        "            })\n",
        "\n",
        "    if len(cluster_stats) == 0:\n",
        "        return []\n",
        "\n",
        "    # Convert to DataFrame for sorting.\n",
        "    clusters_df = pd.DataFrame(cluster_stats)\n",
        "    clusters_df = clusters_df.sort_values(by=['labeled_count', 'cluster_size'], ascending=[True, False])\n",
        "    sorted_cluster_ids = clusters_df['cluster_id'].tolist()\n",
        "\n",
        "    queries = []\n",
        "    for i in range(budget):\n",
        "        # Cycle through sorted cluster ids\n",
        "        cluster_id = sorted_cluster_ids[i % len(sorted_cluster_ids)]\n",
        "        row = clusters_df[clusters_df['cluster_id'] == cluster_id].iloc[0]\n",
        "        candidate_indices = row['unlabeled_idx']\n",
        "        if len(candidate_indices) == 0:\n",
        "            continue\n",
        "        candidate_embeddings = unlabeled_embeddings[candidate_indices]\n",
        "        effective_K = min(20, candidate_embeddings.shape[0])\n",
        "        typ_scores = compute_typicality_nn(candidate_embeddings, K=effective_K)\n",
        "        best_local_idx = candidate_indices[np.argmax(typ_scores)]\n",
        "        queries.append(unlabeled_ids[best_local_idx])\n",
        "        # Remove the selected index from the candidate list\n",
        "        updated_candidates = [idx for idx in candidate_indices if idx != best_local_idx]\n",
        "        row_index = clusters_df.index[clusters_df['cluster_id'] == cluster_id][0]\n",
        "        clusters_df.at[row_index, 'unlabeled_idx'] = updated_candidates\n",
        "\n",
        "    return queries\n",
        "\n",
        "def show_selected_images(dataset, selected_ids, num_to_show=20):\n",
        "    \"\"\"\n",
        "    Visualize selected images from the dataset.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    images_to_plot = selected_ids[:num_to_show]\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i, idx in enumerate(images_to_plot):\n",
        "        img, label = dataset[idx]\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        # Adjust image for display; adjust these values based on your normalization\n",
        "        img = img * 0.2 + 0.5\n",
        "        plt.subplot(1, num_to_show, i + 1)\n",
        "        plt.imshow(np.clip(img, 0, 1))\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{label}\")\n",
        "    plt.suptitle(\"TypiClust Selected Images\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fuAr3ui41cDG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "simclr_full_embeddings = extract_embeddings(simclr_model, full_loader)\n",
        "embedding_file = \"saved_embeddings/simclr_cifar10_embeddings.npy\"\n",
        "os.makedirs(\"saved_embeddings\", exist_ok=True)\n",
        "np.save(embedding_file, simclr_full_embeddings)\n",
        "print(\"Full embeddings saved with shape:\", simclr_full_embeddings.shape)"
      ],
      "metadata": {
        "id": "YZpuNdaczdET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f9fcdd-e3fb-4aa2-cee8-b4b022bd63dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3d6d7d243f67>:48: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  feats = feats / norms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full embeddings saved with shape: (50000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 7 â€“ Fully Supervised Active Learning Loop\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "5iPItg8z1dNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def typiclust_selection(unlabeled_embeddings, unlabeled_ids, budget, max_clusters, labeled_embeddings, labeled_ids):\n",
        "    \"\"\"\n",
        "    Use the TypiClust (TP CRP) batch selection method.\n",
        "    \"\"\"\n",
        "    return typiclust_pool_selection_batch(\n",
        "        unlabeled_embeddings, unlabeled_ids, budget, max_clusters,\n",
        "        labeled_embeddings, labeled_ids\n",
        "    )\n",
        "\n",
        "def random_selection(unlabeled_embeddings, unlabeled_ids, budget, max_clusters, labeled_embeddings, labeled_ids):\n",
        "    \"\"\"\n",
        "    Randomly select 'budget' samples from the current unlabeled pool.\n",
        "    \"\"\"\n",
        "    if len(unlabeled_ids) <= budget:\n",
        "        return unlabeled_ids\n",
        "    return random.sample(unlabeled_ids, budget)"
      ],
      "metadata": {
        "id": "0OB7rryo5T9U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def get_model_hash(model):\n",
        "    \"\"\"Returns a hash of the modelâ€™s parameters (before training).\"\"\"\n",
        "    m = hashlib.md5()\n",
        "    for param in model.parameters():\n",
        "        m.update(param.detach().cpu().numpy().tobytes())\n",
        "    return m.hexdigest()"
      ],
      "metadata": {
        "id": "JjIpphyJfE5j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "budget_per_iter = 10\n",
        "num_iterations = 6\n",
        "def run_al_loop_cached(selection_method, simclr_embeddings,\n",
        "                       num_iterations=6, budget=10, max_cluster=500,run_seed=42, base_state=None):\n",
        "    \"\"\"\n",
        "    Runs the fully supervised active learning (AL) loop using cached SimCLR embeddings.\n",
        "\n",
        "    Instead of performing a separate initial selection, we start with a fixed small initial pool.\n",
        "\n",
        "    Args:\n",
        "        selection_method (str): \"typiclust\" or \"random\".\n",
        "        initial_labeled (int): Number of initial labeled examples (we simply take the first N indices).\n",
        "        simclr_embeddings (np.ndarray): Precomputed embeddings for the entire training set.\n",
        "        num_iterations (int): Number of AL iterations.\n",
        "        budget (int): Number of examples to query per iteration.\n",
        "        max_cluster (int): Maximum number of clusters for the clustering-based selection.\n",
        "\n",
        "    Returns:\n",
        "        performance_history (list): Test accuracy at each iteration.\n",
        "    \"\"\"\n",
        "    # Use a fixed initial pool: for example, the first `initial_labeled` indices.\n",
        "    all_indices = list(range(len(train_dataset)))\n",
        "    labeled_indices = []\n",
        "    unlabeled_indices = all_indices.copy()\n",
        "\n",
        "    performance_history = []\n",
        "    # Set the seed for this run.\n",
        "    seed_everything(run_seed)\n",
        "    np.random.seed(run_seed)\n",
        "    random.seed(run_seed)\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"\\n=== AL Iteration {iteration+1}/{num_iterations} ===\")\n",
        "        print(f\"Labeled pool size: {len(labeled_indices)}; Unlabeled pool size: {len(unlabeled_indices)}\")\n",
        "\n",
        "        current_unlabeled_emb = simclr_embeddings[unlabeled_indices]\n",
        "        current_labeled_emb = simclr_embeddings[labeled_indices]\n",
        "        current_unlabeled_ids = unlabeled_indices.copy()\n",
        "\n",
        "\n",
        "        # Selection step: choose new indices to query.\n",
        "        if selection_method == \"typiclust\":\n",
        "            selected_pool = typiclust_pool_selection_batch(\n",
        "                unlabeled_embeddings=current_unlabeled_emb,\n",
        "                unlabeled_ids=current_unlabeled_ids,\n",
        "                budget=budget,\n",
        "                max_clusters=max_cluster,\n",
        "                labeled_embeddings=current_labeled_emb,\n",
        "                labeled_ids=labeled_indices,\n",
        "                random_state=run_seed\n",
        "            )\n",
        "        elif selection_method == \"random\":\n",
        "           selected_pool = random_selection(\n",
        "              current_unlabeled_emb,\n",
        "              current_unlabeled_ids,\n",
        "              budget,\n",
        "              max_cluster,\n",
        "              current_labeled_emb,\n",
        "              labeled_indices\n",
        "           )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown selection method\")\n",
        "\n",
        "        print(\"Selected new sample indices:\", selected_pool)\n",
        "        labeled_indices.extend(selected_pool)\n",
        "        unlabeled_indices = list(set(unlabeled_indices) - set(selected_pool))\n",
        "            # Create a DataLoader for the current labeled set.\n",
        "        labeled_subset = Subset(train_dataset, labeled_indices)\n",
        "        labeled_loader = DataLoader(labeled_subset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "            # Train a fresh ResNet-18 from scratch on the labeled set.\n",
        "        model_fs = get_resnet18_fully_supervised(num_classes=10)\n",
        "        print(\"Training ResNet-18 on labeled set...\")\n",
        "        if base_state is not None:\n",
        "            model_fs.load_state_dict(copy.deepcopy(base_state))\n",
        "            model_hash = get_model_hash(model_fs)\n",
        "            print(f\"Model hash before training (Iteration {iteration+1}): {model_hash}\")\n",
        "        model_fs = train_model(model_fs, labeled_loader, num_epochs=100)\n",
        "\n",
        "            # Evaluate on the test set.\n",
        "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "        test_acc = evaluate_model(model_fs, test_loader)\n",
        "        print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "        performance_history.append(test_acc)\n",
        "\n",
        "\n",
        "    print(\"\\nAL Loop Completed.\")\n",
        "    print(\"Test Accuracy History:\", [f\"{acc*100:.2f}%\" for acc in performance_history])\n",
        "    return performance_history\n"
      ],
      "metadata": {
        "id": "Z9K09cI21dzi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "n_runs = 2  # Set the number of runs you want\n",
        "selection_methods = [\"typiclust\",\"random\"]\n",
        "all_results = {method: [] for method in selection_methods}\n",
        "\n",
        "for run in range(n_runs):\n",
        "    print(f\"\\n=== Run {run+1}/{n_runs} ===\")\n",
        "    run_seed = 45 + run  # new seed per run\n",
        "    seed_everything(run_seed)\n",
        "\n",
        "\n",
        "    # Save the initial model weights for this run:\n",
        "    base_model = get_resnet18_fully_supervised(num_classes=10)\n",
        "    base_state = copy.deepcopy(base_model.state_dict())\n",
        "\n",
        "    for method in selection_methods:\n",
        "        print(f\"\\nRun {run+1}, Method: {method.upper()}\")\n",
        "\n",
        "        # ðŸš« Removed initial selection logic entirely\n",
        "\n",
        "        # Run the AL loop with fixed initial pool and consistent seed\n",
        "        acc_history = run_al_loop_cached(\n",
        "            selection_method=method,\n",
        "            simclr_embeddings=simclr_full_embeddings,    # use precomputed embeddings\n",
        "            num_iterations=num_iterations,\n",
        "            budget=budget_per_iter,\n",
        "            max_cluster=max_clusters,\n",
        "            run_seed=run_seed,  # pass run_seed to clustering functions if needed\n",
        "            base_state=base_state  # pass the base model weights\n",
        "        )\n",
        "        all_results[method].append(acc_history)\n",
        "\n"
      ],
      "metadata": {
        "id": "CKUvg18mD5bU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "5d6c7e7f-05ad-41b4-ae8f-27fdcf2818b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Run 1/2 ===\n",
            "\n",
            "Run 1, Method: TYPICLUST\n",
            "\n",
            "=== AL Iteration 1/6 ===\n",
            "Labeled pool size: 0; Unlabeled pool size: 50000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ffa0432527b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Run the AL loop with fixed initial pool and consistent seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         acc_history = run_al_loop_cached(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mselection_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0msimclr_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimclr_full_embeddings\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# use precomputed embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1bf1514b8129>\u001b[0m in \u001b[0;36mrun_al_loop_cached\u001b[0;34m(selection_method, simclr_embeddings, num_iterations, budget, max_cluster, run_seed, base_state)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Selection step: choose new indices to query.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselection_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"typiclust\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             selected_pool = typiclust_pool_selection_batch(\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0munlabeled_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_unlabeled_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0munlabeled_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_unlabeled_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b1321155f150>\u001b[0m in \u001b[0;36mtypiclust_pool_selection_batch\u001b[0;34m(unlabeled_embeddings, unlabeled_ids, budget, max_clusters, labeled_embeddings, labeled_ids, min_cluster_size, random_state)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcandidate_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabeled_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0meffective_K\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtyp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_typicality_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffective_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mbest_local_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_local_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b1321155f150>\u001b[0m in \u001b[0;36mcompute_typicality_nn\u001b[0;34m(cluster_features, K)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnbrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Exclude the zero distance to self (first column)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mavg_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_precomputed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 X = validate_data(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# error message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfirst_pass_isfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_pass_isfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process and save results\n",
        "typiclust_arr = np.array(all_results[\"typiclust\"])\n",
        "random_arr    = np.array(all_results[\"random\"])\n",
        "\n",
        "mean_tpcr = typiclust_arr.mean(axis=0)\n",
        "se_tpcr   = typiclust_arr.std(axis=0, ddof=1) / math.sqrt(n_runs)\n",
        "mean_rand = random_arr.mean(axis=0)\n",
        "se_rand   = random_arr.std(axis=0, ddof=1) / math.sqrt(n_runs)\n",
        "\n",
        "np.savetxt(f\"{results_dir}/typiclust_runs.csv\", typiclust_arr, delimiter=\",\", fmt=\"%.4f\")\n",
        "np.savetxt(f\"{results_dir}/random_runs.csv\", random_arr, delimiter=\",\", fmt=\"%.4f\")\n",
        "\n",
        "with open(f\"{results_dir}/summary.csv\", \"w\", newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Iteration\", \"Typiclust_Mean\", \"Typiclust_SE\", \"Random_Mean\", \"Random_SE\"])\n",
        "    for i in range(num_iterations):\n",
        "        writer.writerow([i + 1, mean_tpcr[i], se_tpcr[i], mean_rand[i], se_rand[i]])\n",
        "\n",
        "# Plotting with cumulative budget\n",
        "cumulative_budget = np.arange(1, num_iterations + 1) * budget_per_iter\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(cumulative_budget, mean_tpcr * 100, yerr=se_tpcr * 100, fmt='-o', capsize=5, label=\"Typiclust\")\n",
        "plt.errorbar(cumulative_budget, mean_rand * 100, yerr=se_rand * 100, fmt='-s', capsize=5, label=\"Random\")\n",
        "plt.xlabel(\"Cumulative Labeled Budget\")\n",
        "plt.ylabel(\"Test Accuracy (%)\")\n",
        "plt.title(\"Active Learning Performance (Fully Supervised with Self-Supervised Embeddings)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{results_dir}/performance_plot_cumulative_budget.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JvQMjUFXb4Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Statistical Evaluation\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.metrics import auc\n",
        "import pandas as pd\n",
        "\n",
        "p_values = [ttest_rel(typiclust_arr[:, i], random_arr[:, i]).pvalue for i in range(num_iterations)]\n",
        "\n",
        "# Compute AUC (area under curve)\n",
        "iterations = np.arange(1, num_iterations + 1)\n",
        "auc_typiclust = auc(iterations, mean_tpcr)\n",
        "auc_random    = auc(iterations, mean_rand)\n",
        "\n",
        "# Save detailed stats\n",
        "stats_df = pd.DataFrame({\n",
        "    \"Iteration\": iterations,\n",
        "    \"Typiclust_Mean\": mean_tpcr,\n",
        "    \"Typiclust_StdErr\": se_tpcr,\n",
        "    \"Random_Mean\": mean_rand,\n",
        "    \"Random_StdErr\": se_rand,\n",
        "    \"p-value\": p_values\n",
        "})\n",
        "stats_df.to_csv(f\"{results_dir}/statistical_summary.csv\", index=False)\n",
        "\n",
        "# Save AUC values\n",
        "auc_df = pd.DataFrame({\n",
        "    \"Method\": [\"Typiclust\", \"Random\"],\n",
        "    \"AUC\": [auc_typiclust, auc_random]\n",
        "})\n",
        "auc_df.to_csv(f\"{results_dir}/auc_summary.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Detailed statistical summary and AUC summary saved.\")\n"
      ],
      "metadata": {
        "id": "7MCFaFpjpAza"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1ctgBTpJAkGq_uXFVOBYsvoN9lQaIlMEy",
      "authorship_tag": "ABX9TyO06Fu0w9oFaBjs0/WGtvEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}